## A Simple Coding Agent Eval

This repository documents the outputs of various coding agent for evaluations. The goal is to build intuition about the capabilities and limitations of popular coding agents.

This report is by no means scientific. The results are intended as a broad overview, not definitive proof of performance - interpret them accordingly.


## Methodology

The same [prompt](prompt.md) is used for all tests. For each run, we ask the coding agent to to one-shot the challenge. 

To isolate the context, we run each coding agent in an empty folder called "workspace". This way the artifacts from other models do not interfere with the current model.

There's a script that creates a symlink of the `images` folder and `image_catalog.json` for each workspace.

After each run, we document the logs called "output.txt" in the directory one level up.

Please note that the code agents are not run in a fully isolated environment. There are user level prompts such as user level CLAUDE.md could influence the results.

## The Artifacts

To view the generated artifact, which is just a HTML file, open it in a browser.

Some questions to ask:

- Is the prototype aesthetically pleasing?
- Are the UI elements positioned correctly? This can be especially challenging for some LLMs.

## Attribution

All images in this project are generated by Grok AI.